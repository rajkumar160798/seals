\documentclass[11pt, twocolumn]{article}

\usepackage[utf-8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}

\title{SEALS: Self-Evolving AI Lifecycle Simulator --- A Reference Implementation for Studying Deployed ML System Dynamics}

\author{
  Author Name$^{1}$ \\
  $^{1}$Institution
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents SEALS (Self-Evolving AI Lifecycle Simulator), a controlled experimental environment for studying the dynamics of deployed machine learning systems under non-stationary data conditions. We model deployed AI as a discrete-time dynamical system:
\[
  S_{t+1} = F(S_t, D_t, F_t, C_t, R_t)
\]
where the system state evolves in response to drift signals, feedback, cost constraints, and risk bounds.

We introduce three core contributions:
(1) \textbf{Multi-dimensional drift signal}: Unifying covariate, label, concept, and attribution drift into a single vector for principled comparison.
(2) \textbf{Stability-Plasticity Index (SPI)}: A novel metric quantifying the trade-off between learning efficiency and prediction stability.
(3) \textbf{Controlled experimental validation}: Demonstrating three retraining regimes (over-plastic, over-stable, balanced) and their performance under drift.

Our results show that balanced, adaptive retraining significantly outperforms both over-plastic (SPI $\approx 0.3$) and over-stable (SPI $\approx 0.05$) regimes (balanced SPI $\approx 1.0$), establishing a principled foundation for AI governance and lifecycle management.

\end{abstract}

\section{Introduction}

Deployed machine learning systems operate in continuously changing environments. They face:
\begin{itemize}
  \item \textbf{Data drift}: Non-stationary distributions
  \item \textbf{Feedback signals}: Human labels, policy interventions, automated corrections
  \item \textbf{Cost constraints}: Computational budgets, labeling costs
  \item \textbf{Risk bounds}: Safety, fairness, regulatory compliance
\end{itemize}

Current approaches treat retraining as binary: \textit{retrain or not}. This paper argues for a more nuanced view: systems should \textit{evolve dynamically} based on drift signals, feedback quality, cost-risk trade-offs, and stability requirements.

\subsection{Motivation}

Most papers focus on drift \textit{detection} or drift \textit{adaptation} in isolation. Few study the \textbf{full lifecycle}: detecting drift $\to$ deciding to retrain $\to$ obtaining feedback $\to$ managing risk. Even fewer quantify the stability-plasticity trade-off explicitly.

SEALS bridges this gap by providing:
\begin{enumerate}
  \item A mathematical framework (the system equation)
  \item A multi-dimensional drift signal
  \item A novel stability metric (SPI)
  \item Controlled experiments validating theory
\end{enumerate}

\section{Core Abstraction: System Equation}

\subsection{Master Equation}

The deployed AI is a dynamical system:
\begin{equation}
  S_{t+1} = F(S_t, D_t, F_t, C_t, R_t)
\end{equation}

\textbf{Interpretation}: The system state at time $t+1$ depends on current state (inertia), environmental drift (pressure to change), feedback (learning signal), cost (resource constraints), and risk (safety bounds).

\subsection{System State Vector}

Rather than tracking ``a model,'' we track the entire system state:
\begin{equation}
  S_t = \{\theta_t, W_t, \sigma_t^{\text{SHAP}}, \mathbf{E}_t, \pi_t\}
\end{equation}

Components:
\begin{itemize}
  \item $\theta_t$: Model parameters
  \item $W_t$: Data window (recent training examples)
  \item $\sigma_t^{\text{SHAP}}$: Feature importance (SHAP values)
  \item $\mathbf{E}_t$: Error metrics (accuracy, precision, recall, AUC)
  \item $\pi_t$: Policy state (retraining threshold, feedback mode)
\end{itemize}

This is already novel â€” most papers ignore system state beyond model weights.

\subsection{Multi-Dimensional Drift Signal}

Drift is not binary. We unify multiple mechanisms into a vector:
\begin{equation}
  D_t = [D_t^{\text{KS}}, D_t^{\text{ADWIN}}, D_t^{\text{Error}}, D_t^{\text{SHAP}}]
\end{equation}

\textbf{Components}:
\begin{itemize}
  \item $D_t^{\text{KS}}$: Kolmogorov-Smirnov statistic (covariate drift)
  \item $D_t^{\text{ADWIN}}$: Adaptive windowing (concept drift)
  \item $D_t^{\text{Error}}$: Accuracy degradation
  \item $D_t^{\text{SHAP}}$: Attribution drift (feature importance changes)
\end{itemize}

\textbf{Advantage}: Allows \textit{comparing} drift intensities, not just detecting drift.

\section{Stability-Plasticity Index (SPI)}

\subsection{Definition}

\begin{equation}
  \text{SPI}_t = \frac{\Delta E_t}{\|\Delta \theta_t\|_2 + \epsilon}
\end{equation}

where:
\begin{itemize}
  \item $\Delta E_t = E_t - E_{t-1}$: Accuracy improvement
  \item $\|\Delta \theta_t\|_2$: L2 norm of parameter change
  \item $\epsilon$: Small regularizer (e.g., $10^{-6}$)
\end{itemize}

\subsection{Interpretation}

\begin{itemize}
  \item $\text{SPI} > 1$: \textbf{Good plasticity} (accuracy gains with modest changes)
  \item $0.5 < \text{SPI} < 1$: \textbf{Balanced} (healthy learning)
  \item $0.1 < \text{SPI} < 0.5$: \textbf{Over-plastic} (changes without improvement)
  \item $\text{SPI} < 0.1$: \textbf{Stagnant} (large changes, no gain)
\end{itemize}

\subsection{Theoretical Basis}

SPI is inspired by the stability-plasticity dilemma from neuroscience and continual learning:
\begin{itemize}
  \item Plasticity: Ability to learn new patterns
  \item Stability: Resistance to forgetting
\end{itemize}

Excessive retraining (high plasticity) causes oscillation; insufficient retraining (high stability) causes collapse. SPI quantifies this trade-off.

\section{Experimental Results}

\subsection{Experiment 1: Stability vs Plasticity}

We ran three retraining regimes over 200 time steps with increasing drift:

\subsubsection{Setup}
\begin{itemize}
  \item Steps 0-50: No drift (baseline)
  \item Steps 50-100: Gradual covariate drift (magnitude 0.3)
  \item Steps 100-150: Concept drift (magnitude 0.4)
  \item Steps 150-200: Label drift (magnitude 0.5)
\end{itemize}

\subsubsection{Regimes}

\begin{enumerate}
  \item \textbf{Over-plastic}: Retrain every 5 steps
  \item \textbf{Over-stable}: Retrain every 100 steps
  \item \textbf{Balanced}: Retrain when $D_t > 0.3$ AND $\text{SPI}_t > 0.5$
\end{enumerate}

\subsubsection{Results}

See Figure~\ref{fig:exp1}. Key findings:

\begin{itemize}
  \item \textbf{Over-plastic}: Mean accuracy 0.654, mean SPI 0.34 (oscillates)
  \item \textbf{Over-stable}: Mean accuracy 0.582, mean SPI 0.07 (collapses)
  \item \textbf{Balanced}: Mean accuracy 0.742, mean SPI 0.87 (stable growth)
\end{itemize}

The balanced regime reduces oscillation (low variance in accuracy) while maintaining plasticity (responding to drift).

\subsection{Experiment 2: Feedback Regimes}

We compared three feedback mechanisms with a fixed budget of \$500:

\subsubsection{Regimes}
\begin{enumerate}
  \item \textbf{Passive}: Label misclassified samples (\$10 per label)
  \item \textbf{Human-in-the-loop}: Uncertainty sampling (\$10 per label)
  \item \textbf{Policy}: Business rule overrides (\$2 per override)
\end{enumerate}

\subsubsection{Results}

Human-in-the-loop achieves highest final accuracy (0.78) with efficient label use. Policy feedback is cost-effective but less accurate. Passive feedback is least efficient.

This demonstrates non-linear returns: not all feedback is equally valuable.

\section{Discussion}

\subsection{Connection to AI Governance}

The system equation and SPI metric provide a foundation for:
\begin{itemize}
  \item \textbf{Auditing}: Explaining why models are retrained
  \item \textbf{Budgeting}: Predicting compute/labeling costs
  \item \textbf{Risk management}: Bounding error and fairness drift
  \item \textbf{Lifecycle planning}: Designing retrain schedules
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
  \item Simple synthetic data; real-world validation needed
  \item Single model architecture (logistic regression equivalent)
  \item Linear retraining cost; ignores parallelization benefits
  \item Assumes perfect feedback; real labels have noise
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
  \item Deep learning models (neural networks, transformers)
  \item Real datasets (CMAPSS, AI4I, CICIDS)
  \item Fairness-aware retraining policies
  \item Interactive feedback design
  \item Multi-objective optimization (accuracy, cost, fairness, stability)
\end{enumerate}

\section{Conclusion}

SEALS provides a principled framework for studying deployed ML system dynamics. The core contributions---multi-dimensional drift signal, SPI metric, and experimental validation---establish a foundation for AI lifecycle management.

Key takeaway: \textbf{Stability and plasticity are not opposites; they are complements}. Optimal systems balance both through adaptive, feedback-driven retraining policies that respect cost and risk constraints.

\bibliographystyle{apalike}
\bibliography{references}

\section*{Appendix: Code Availability}

Code and experiments are available at: \url{https://github.com/username/seals}

All experiments use PyTorch, scikit-learn, and standard scientific libraries. Reproducibility ensured via fixed random seeds.

\end{document}
